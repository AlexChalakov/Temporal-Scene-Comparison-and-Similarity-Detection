DATA STRUCTURING

All right. So for each image, we are going to check out the image and the label and the relationships to that image. Okay. Okay. So suppose you have image one, and that has label, label one for family one, you know, so you would identify every image that is related to image one and find out the label of those images, and then you change it to be the same LA label as image one. 

So that basically says that they're all matched mm-hmm. but you don't change that label immediately because what you'll do is you have to do it in an iterative approach.  So what you've already, what you do initially is you take a snapshot of the current image labels because you have to go one image after the other, after the other. You Have to go through all of them and then change. 

So you can't change the other labels while you're still going through it, or your code's gonna crash. Yeah. Um, so you sort of make a note of all the changes that you're going to make, and then when you get to the end, you make those changes, but then that's not enough. So you record also the number of changes that you made just for, for reference. Okay. But that's not enough, because there could be some deeper relationships there. So just because, so that will link everything two image one, but what about the things that are linked to two, image two? 

Mm-hmm. as well. Um, so you have to do another pass. So you basically, at the end of it, you make all of your label changes and there may be some conflicts in those changes as well. Um, and then you have another pass. You go through all of the images, you do exactly the same thing again, no, the number of changes. And if you're making any, if you make any changes at all, you do another iteration. Okay. And then you do another one. And you keep going until you reach a point where you go through the whole thing and there are no changes to make because all the labels are done. And that means that everyone's linked, in this case, it took five iterations. Five 
Iterations, 

And once the code was written, the code took hours. But once the code was written, and it's such a simple stupid code, but once the code was written, it took maybe one second to run to do all of that, which is typical. Um, but that did, but that basically gets your, um, images relabeled. And then from that, the rest of this is easy. This is just all about building up the CSV file, Building up the families. So families, folders. 

so one departure from the methodology that they're using for identification is that they iden, you know, they, they have F 0, 0, 0. Yeah. And then they just pick one family for validation. Well, if you actually run their code as it is, which could be a good idea actually, just to, to see, you'll find actually their results are not very good.  um, and their results are not very good, partly because it's a bit of a silly problem. because they're relying on the assumption that you can identify you related family members by, but just random pictures, which is, is, is kind of unreasonable.  um, and it's experimental. But the one of the key problem really is that they have only one family for validation. That's a very small validation set. So what we've done here is we are changing it because we wanna identify specifically training, validation, and testing families. Okay. Um, so in this case, the training families, um, are going to be 60% of the families. Yeah. And they're randomly chosen to be training. 

From the remaining 40%, you randomly choose half of those for validation. And then everything else is for testing. 
So for the running 40, you choose 20 to be for validation. 

What it remains goes For testing.  So a good way to do this kind of split is to do it with a random permutation, which is done somewhere. Yeah, there it is. So we create a random permutation, which is just basically, um, take all of the in-depth indices from one up to however many and then scramble them. Okay. Um, and then you can say, I take the first 60% of the scrambled ones and that gives you, that gives you your training set. So the way we're identifying them, this is one departure is we, we're not using F anymore. We're not using this f o nine, we're using TF for training v F about validation and T TF for testing, which is just much more explicit. It's a lot easier and it makes a lot more sense. And then we're, uh, copying the images over two a new folders. So this is the testing, I think, oh no images reconstruct. 

So if we look in at images structured folder Are the creative tests now. Yep. Yeah. So we got, all right, so these are the testing ones. These are the training ones and these are the validation. Okay. There we're, so the only thing, so that sort of sets them up, such that your relationships file, which is now 
Medium, sorry, this is is called medium just cuz it's the name of the file  um, is exactly the same format as what they use. They use it. Yeah. Um, so we are listing all of the relationships here. They're not necessarily all used, the tiers are all used for training, the easy for validation and so on. Um, so it basically uses all of these specified relationships for the training.

 Um, and then creates the, um, adversarial relationships. The ones where there's no match for every training image that you use. It will create a No match at random by picking something from a different family because that's the only guarantee that there's no match. So that's a really important point as well. Okay. Okay. 



SIAMESE NETWORKS

So I tweaked the code, it's the BW one from them. 
it's just changing the concept online. 43 and 44. Um, you know, instead of having this val families is F O nine business, we wanna just be specific that the training families are the TRFs, the Val families are the VAs. Um, and then we pull, this has to be slightly tweaked then the training images and the VAL images, um, is just saying it's the ones that satisfy this pattern. Um, and then file names. Of course we want it to read the medium relationships file instead of their own file. And we want it to use our data folders instead of their data folders. And also for the testing, we want it to use our testing CSV file, our testing images and we want it to output our testing name as well for the CSV file so we're not messing around. So it just keeps it nice and isolated. So it means then that for us, for our testing data, what we got, we've got sample submission medium. So this is our testing index. So you can see how it's created as that 1 0 1 0 10. So we're just going for every image and we're choosing a matched image and we're choosing a no matched image. That's it. So that's A ground truth. And then the results of the model were outputting into submission medium. So this is the output. It may or may not be any good. It trains extremely well. 
Nice. 

But it doesn't validate all that well. Okay. So the blue one is your training, your orange one is the validation. So even the training, so the training I think is maybe overfitting here because the training actually does hit a hundred percent. It gets very close to a hundred percent a couple times and then it hits a hundred percent. Yeah. At one point, so you've got very, you've got fantastic training accuracy, but your validation accuracy actually goes over 80% but then starts to decline. 
Um, so the way that we'd, what we'd normally do with this is we'd output two different things. Uh, well three different things. We'd output the CSV files, showing the results as we go along because this is how we can generate the first. 
We we're showing them what we got. 


we'd output what's going on at the end of the model once it completely finishes. And we'd also output whatever was the best validation score during that model, during that training, we'd output that one. So, um, we can see here that uh, EPOCH 99, it's got 62% validation. But what is the best validation result? 84.83%, which is a lot better. So it is possible. Um, see one issue with saying these networks is that you've got, um, you've got almost a conflicting scenario. It's almost like joint optimization. Um, in that it can be very difficult to get it to converge in a generalizable way. Okay. Right. Um, cuz it's a bit like you're trying to solve one problem and then solve another one at the same time. Yeah. So what can happen with that is exactly what we've got that it increases and then starts to decrease. 
So 84.83, let's see, where does it hit the maximum?
Is that is at the beginning? I think 
11 EPOCH. I wouldn't necessarily take that as, I mean it is the best one. But given the training accuracy I would say is a little bit lower there to take that as trustworthy.  But it is the best validation, accuracy. So if we have a look at some of the testing data, I mean idea is some of the, if it gets low validation, you can expect probably low testing as well. But then again, the first one is it related? 

Yes. It got it right. Second one, no. Got it. Right. This one, yeah, it got this one wrong as well.Okay, so this one is wrong in the, in the same family. Mm. So this should be related. Oh, they're not just in the same family. If we compare it with these two, it's exactly the same. You say just reverse.I mean it can happen theoretically. Um, but that's interesting. So it is definitely directional. This one is different family. Different family, different families. See these are all zeros but they're all different families. So it's actually getting these correct.  um, I'm trying to find one that should be a match. all of these are correct. I mean all of these are correct, but they're, I want one that's actually supposed to be a match. Um, it might actually be worth looking at how the testing data is generated. Cause I did write this fairly quickly, oh wait, we've got a one there. What's this? So it's saying that this two are match. I mean it could be interesting. I mean what what is always interesting is to have a look at these, um, to do an analysis of um, what are matches, which ones are picked up as matches and which ones are not. Because it's possible that the ones that are being incorrectly picked up as matches are actually similar in some way. 


But then again it is in, it will be interesting. One 30 and one, a lot of these are being matched to one, which is very interesting. And it could well be, I wonder if that oh, oh one image is I think is what the one of the bridge. Um, so if that's the one of the bridge that could include quite a lot of stuff that actually means that all of these matches are actually reasonable. Yeah. Yeah. That's one of the problems of Siammese. Um, I mean one, oh, so one thing, I mean it would be good anyway to, 
Can we use that to our advantage? Cuz like we, we known, like from what I've read, cause I'm reading like Siamese papers right now. Yep. They outlined those disadvantages. they outlined what happened. Can we do that as well? Like say, you know, we conducted this and we realized that Siamese are not that reliable because of this, this and that. Um, yeah. I mean I wouldn't even necessarily frame it as not being reliable. Yeah. 

Yeah. Yeah. So yeah, so that's, as, that's actually what you really should do. Okay. Is look at um, a detailed scientific analysis of the results, um, and look at some comparisons, do an examination of it. So it's one thing what you want to present is a quantitative analysis, which is like a sensitively specificity out of this. But for this especially qualitative analysis is really important. So you could take a random sample. Um, you could even, it's important enough that you could even have a page of your dissertation literally to show, um, however many, you could probably fit four side by side by side four columns and six rows, eight rows even. 
Um, which would give you 16 examples. So you could pick six random matches from results and 16 random matching results and pairs and 16 random NO match pairs. Um, and you could have a zoom in on those and say, well actually, you know, these are really regarded as a match, but you can kind of see why. It's kind of reasonable. Um, so you can look at that or there might be somewhere it says this is a match, but really it doesn't look anything like a match. 

Um, another interesting concept with siese, one of the things I like about Siemese is that, you know, if you're doing say classification, what you're learning is how to interpret a single image, um, or a single type of image to learn about it. In Siamese you're learning, you're learning how to compare, you're learning about image interpretation for sure. But your main thing you're learning is how to compare images. 
 

It's probably here as well. Like bull or n y uh, one E bedroom. Oh, there you go. So yes. So that's why they're like a match. 
Oh, that's a good point. 
Yeah. That's what I just saw. That's a good point. That would be interesting to see cuz it's, It doesn't match to any of the other ones cuz they're not bedrooms. But when it comes to another bedroom, even if it's not the same exact same, you'd probably recognize it. there's a bedroom still and that's why it matches it. Yeah. So it could well be that you can say that there is some kind of gray area. There's different, because there's different, and again it comes back to this definition of saying, well there's different layers of matches, there's different levels of matches. Are we matching it cuz it's exactly the same office or because it's um, an office space.  you know, um, cuz it could well be that those two bedrooms look the same. Um, ooh. And actually for Siamese, there's a point that I didn't think, think about. So for Siamese you can always consider candidate matches as well. 

So often if you're working in Siamese, um, if you're working in Siamese, um, networks, you're usually doing some kind of identification, some kind of matching task. Um, and you don't necessarily want the first match. What you're after is the most likely matches. Okay. 
Now that is a very different prospect of an experiment. Um, and when I say most likely, it's the ones with the highest scores, the highest matching scores. So you know, like here we've got binary output between zero, well we've got zero at one. you can change your model so that it won't give us, so the zero one is essentially, um, a final threshold output. So the actual, you know, um, if you're talking about uh, multilayer perceptron Yeah. Models, you've got your two strands of the model that come down and they'll give some value. Who the hell knows what the value of that means? You have to have some interpretation layer, um, at the end, like a soft max layer or something that will then give you, that will take this percentage and turn it into a, an answer is activation actually is what that is. Um, so activation turns it into an actual value. A zero war one. Well if we stop it just before that value and say look, we want the percentage, then that in theory is a percentage of how is similar are these two things. and you can actually consider that percentage then. Um, so what you could say, you could reframe the problem. So what we are doing is we choose, we, we are just defining a set of testing options and we're asking you to do a classification for us.

Suppose we change that and we say look for this image here, this FO 2 25 NYU bedroom 0 56 1, um, under score 1 32 jpeg. We want to compare that with every single other image in the testing set. and what we want for every single possible combination is a matching score. And then we're gonna choose the top 10 matches or the top five matches. Okay. And we are gonna see what's in there because what you can get from that, you can identify, um, it's called top N ranking. Um, so you can identify things like the top one match. So where you're saying, okay, this is the most likely match from within the data set. Is it, is it in the same class on it? you've got top five ranking, which is where you take the the most five, the five most likely ones and see, uh, any of them matches. If they are then good, you're happy. 




so if you sort of build up the dissertation gradually from now is gonna be relatively easy to write. and your experimentation, you'll get to a point where you know the code and you know what you're doing. So it's literally gonna be, oh, I just made this little tweak and click run. Um, so experimentation can continue if you want, right up to week 19 mm-hmm. so there's no crazy, you know, there's not, not not the case that we have to finish experimentation in the next week or something. so for this then, cuz I think at this point though, we need to decide exactly where do we want to target the next thing because there's so much different stuff that we could do.
 
Um, so the first thing you'd wanna do is an analysis of this results. So the results are actually here now. So you'd wanna do a qualitative and a quantitative analysis and actually this, I would probably write it straight into your report while I was writing the analysis. So it's there, um, cuz it will almost, it's gonna end up there. Um, and do a similar thing with your, uh, images. Okay. Your image pairs as well. So you'd take, I would literally take 16 random matching PERS and 16 random no match pairs and just have a nice page, even two pages of cars, in your dissertation to show those because that is actually the qualitative analysis. But 
Did that literally show the pictures and show like, you know, they've been matched cuz of that. 
Okay. Oh yeah, sure. Um, and you could include the zoom ins if you want.

we did talk a while ago about doing, uh, si no. Um, about doing attention, um, with these that might still be doable actually. Um, I dunno if it's good, but it will be based on the qualitative analysis. We'll determine whether or not it's gonna be interesting to do attention. Cuz I think when you look at your matches and your no matches, I think you're gonna see instantly why is it a match and why is it no match? That's true. Um, some of them might be a bit abstract laboratory. Um, and based on that you might want to, to do an attention approach. Cause attention, the idea of attention is should highlight what is making the, what key elements of the image are making this decision. Um, so, you know, if the, it's like the bedroom, if there's just, if it's because there's a bed in the room Yeah. You know, is gonna highlight the bed. But the limitation with attention is the, um, it's might not just be the fact that there's a bed in the room. It could be that the bed is the same style. 
Um, it could be that the bed's at the same angle and that's what attention doesn't tell you. It just basically says, look, this is the relevant pixel. 
So that's kind of, this is, that's kind of the limitation with attention really. Um, it, it tells you pixels, but it doesn't tell you any logic. 

Sift is kind of an easy one to do, so Oh yes, actually. Yes. Sift you can just, Because like in the proposal I've talked a lot about sift. Okay, that's 
Sift. Oh, did we say we do a hybrid? We may have done that. Because sift, the good thing about sift is you can do sift in a day. 
Oh, okay. That could be interesting. C sift is bidirectional. Um, meaning the, you know, like here we've got this one matches with that one, but the other way around it doesn't match. With sift it does. So you could say, look, let's consider free classes of opinion, let's do majority voting here. let's use this direction, that direction and sift. They're all gonna give, that means you're gonna get threee decisions about whether or not it's a match. Okay. Um, and then you'll take the majority decision. Okay. So if by direction only here it says it's a match, then it's a match. That would be a way of combining them.  

first thing to do would be a quantitative and qualitative analysis of these results. So looking at the accuracies. Um, so your results are in these two files. So your output is in the submission medium. Yeah. Um, what is that history 
History dot csv, uhhuh is that one. So this is the, the running of the model and the results. 

So we've got images structured. All right. Yeah. So I would use definitely this structure. Now for sift, it's not really a training approach. Um, it's more of a comparison, image comparison approach without training. So you only need to evaluate it on the T images, the testing images. Um, so I would basically, for sift, you're gonna have to write, you sift, what you'd have to do is write the code to look at the images yourself.  Um, and then you use sift to make the comparison between the images. Okay. Um, so the, your best way to do that is going to be to use, where's it going? Sample submission medium because this is your in index. Oh 
The image pair. Okay, there you go. So this is gonna be an image pairs. So you have to import this and just separate them out by hyphen. sift is all about, um, finding relationships between images. So it will either find those relationships or it won't find them. Yeah. Um, so it's pretty 

So the point of CIF is that if you take a picture of something is look and why you give it two pictures. Um, and what it's looking for is within each image, what are the distinct points within an image. So if you, if I take a picture of this room and chuck it into sift, it's not gonna choose a point in the middle of that wall because there's loads of those. who cares? It's gonna probably choose a pint at the corner of the bookcase. Okay. And say, well this is kind of unique. Okay. So this is a good reference point to consider. Um, and then it's gonna do the same thing in the other image. So it's gonna basically find points that should be unique and consider those for comparison rather than every single pixel and an image. and then it does a comparison of those points with each other. So the, the short list of pints, um, yep. So we'll do a comparison with those and determine if it fits. Those are a match between the images. And then it will also look at not just sort of based on the local pixel intensity or the patterning, but I'll also look at the uh, relationship, um, creation. Okay. So, you know, like, um, if I take a picture of that and then I take a picture of that, so I'm just moving the camera slightly, you're gonna expect all of the matched points are gonna be in a straight, there's gonna be a straight line 

so you'd basically make the decision based on this direction, that direction and sift and then sift is gonna be your explainability concept. It's always worth in any kind of scientific report having something to compare the results. Um, so we can either compare different architectures and I have no idea what architecture we're using for this. Um, what subnetwork, so that may or may not be worth doing is changing the backbone encoder. Okay. But I can't, I'm not sure what is actually it is, I'd have to check and find out which encode is, um, or I would say change the data. So there's two things. So you'd use, you know, this idea of training on faces because that's a more concrete example. Yeah. Um, that's gonna mean a slight change to the code, but not huge. Um, so training on faces and then evaluating on this as a comparison. Okay. Just to see if that helps. And then what, it just gives you, it gives you an extra thing to discuss and it gives you, um, a bit of a table for your results section as well. So you're not just sort of saying, look, these are my results, take a long leave, but you're saying these are my results and I've contrasted it with something else and this is the best approach.  





SECOND TRANSCRIPT


I mean, I have a separate data section. I've started like writing it out, uh, how complicated it is, why it's important. This is so big and everything and, um, yeah, it's going go good so far, but I know, like I, I'll make sure like, you know, like, um, like not even simplify, highlight the importance of the data preparation we've done. Mm-hmm. And the, the work with the data will be done throughout the Whole project.

I mean we know the, the logic behind Siamese networks, um, is that we've got pairs of images and those pairs of images will have either true or false label. 
Yeah. You know, from that point of view it's kind of straightforward. Um, but when it comes to the actual interpretation of the data and the clustering of the data into training and testing and validations sets is very different because you've got hierarchical levels. So I would talk about it from the point of view of hierarchical clustering, um, because you've got, um, you've got the direct relationships that we know about. 

That's the simplest one. Um, so this, um, so it's always good when you're talking about a model or when you're talking about data prep or something that you design as a solution  to start with what is the problem and what are the requirements, and then your solution just hits those requirements. Yeah. All right. Um, so in terms of creating the data set, for example, um, the data set, you know, the medium data set  that I gave you, the easy  Yeah. Ah, and we jumped straight to the medium. 
Yep. Because it was the more interesting one.because the difficult, I don't think it will work. Yeah. Um, and the easy will just No problem. Its easy. Yeah. Yeah. So it could be worth actually as a final step testing the easy data set as well just to see, just for, cuz it's always very important in a reports like this to present some comparisons

Brilliant. And what you can always do in your data, in your sort of data creation type maybe, um, section, um, is you can describe the individual data sets and what is, what is the logic behind using all of these different data sets,  Um, and then to create the overlaps, what we did was we randomly selected, uh, subpar a subsection of the image. So it means you just take some bucks, you put it on the image, um, and then how did we do it? 
We took, yeah. So we took one box, put it on the image, and not just necessarily a box, but it's a box that could be rotated, it could have some sheer, it had some shifting, you know, um, so we extract a scene from an image and then we randomly extracted I think 1000 of us sub images mm-hmm. 
So you do really, you're doing 1000 random, um, shapes all over the image. And then we find which one, one of these, which has, um, 90% overlap with our image, and that one should be the easy case. Okay. Then we find the ones, was it 90 is either 80 or 90? I think 80 probably. Okay. Then we looked at ones that had between 50, I think 40 and 80% overlap. They are the medium ones. And then the cases which had less than 40% overlap, they were the difficult ones. 

Um, so the what that, the reason that we did it that way is we wanted to have a very controlled scenario where we knew exactly the difference. Exactly what is the overlap. So what should be our expectation with these images, you know? Um, so that has, and, and we also, we did um, multiple boxes. 
We didn't just do one box per image, we did multiple. Um, so that comes with significant positive in that we have a very controlled experiment then, but of course the negative, so we had to use multiple because otherwise we don't have enough images.Um, the negative of course is that if you have kind of your central one, you know that this image is gonna be linked to this overlapping one and this overlapping one, but you don't have any logic of over another. These two are linked. Yeah. So because we have no information if they're linked or if they're not linked, we just say nothing about them. Okay. We just say maybe they're linked, but we don't know. Yeah. Right. So that's how they were created. So what you can do is go into some detail about that and say, okay, these are created, um, by placing, you know, by placing this random box, but it's not just a random, it's a random box that's rotate has rotation and share, Okay. As well and being placed in random 
Position. Yeah. That's okay. And now this is a function, do you call it somewhere? What is that? 

But yeah, I researched into, but I only got what I sent you. 
Yeah. So for sift, I think what we'd be thinking about, I mean the logic behind sift, um, is, is actually more from the concept of, uh, registration and photogrammetry.  um, for sift. Um, so image registration means taking two images and co aligning them. Okay. So there's, and there's many, many different ways to do that. Um, so one of them is a kind of key point registration. Okay. So key point means the, supposedly I want to co-align my hands, so I want to overlay them over each other. Okay. So I can overdo it based on pixel intensity. I can base it on whole field defamation, um, or maybe I just base it on key points. So key points could be just picking the joints, defining what these joints are here, defining the corresponding joints here, and then just moving them over each other. Mm-hmm. <affirmative>, you know, it's kind of easy. Um, but of course it has the negative, you have to identify those joints. 
Yeah. Um, so with sift, the concept would be. 

